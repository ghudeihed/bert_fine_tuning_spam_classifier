{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data and view the first couple of rows \n",
    "df = pd.read_csv(\"./dataset/spamdata_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution of the labels\n",
    "df[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train, validation, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_val_text, train_labels, test_val_labels = train_test_split(df[\"text\"], df[\"label\"], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df[\"label\"])\n",
    "\n",
    "# split test_val_text and test_val_labels\n",
    "test_text, val_text, test_labels, val_labels = train_test_split(test_val_text, test_val_labels,\n",
    "                                                                random_state=2018,\n",
    "                                                                test_size=0.5,\n",
    "                                                                stratify=test_val_labels\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model and Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained BERT model and tokenizer\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and Encoding\n",
    "def tokenize_and_encode(text_data, max_seq_len):\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        text_data.tolist(),\n",
    "        max_length=max_seq_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    return torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'bert', 'model', 'tutor', '##ial', ',', 'is', 'a', 'fantastic', 'way', 'to', 'learn', 'we', 'will', 'fine', '-', 'tune', 'bert', 'model']\n",
      "{'input_ids': [[101, 2023, 14324, 2944, 14924, 4818, 1010, 2003, 1037, 10392, 2126, 2000, 4553, 102], [101, 2057, 2097, 2986, 1011, 8694, 14324, 2944, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Sample data \n",
    "text = [\"This bert model tutorial, is a fantastic way to learn\", \"We will fine-tune bert model\"]\n",
    "\n",
    "sent_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(sent_tokens)\n",
    "\n",
    "# Encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "\n",
    "# Print token ids\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Text Sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the training set \n",
    "seq_len = [len(txt.split()) for txt in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins= 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assume train_text, val_text, test_text, train_labels, val_labels, test_labels are correctly initialized\n",
    "train_seq, train_mask = tokenize_and_encode(train_text, max_seq_len)\n",
    "val_seq, val_mask = tokenize_and_encode(val_text, max_seq_len)\n",
    "test_seq, test_mask = tokenize_and_encode(test_text, max_seq_len)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze BERT Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture \n",
    "\n",
    "Adding two fuly connected layers on top of BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Model Architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # Pass the inputs to the model\n",
    "        outputs = self.bert(sent_id, attention_mask=mask)\n",
    "        cls_hs = outputs.last_hidden_state[:, 0, :]  # Take the [CLS] embedding from the last layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghudeihed/anaconda3/envs/XCS330/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model and Move to GPU if available\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Class Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57743559 3.72848948]\n"
     ]
    }
   ],
   "source": [
    "# Compute Class Weights\n",
    "class_wts = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "# convert class weights to tensor\n",
    "weights = torch.tensor(class_wts, dtype=torch.float).to(device)\n",
    "\n",
    "# Define Loss Function\n",
    "cross_entropy = nn.NLLLoss(weight=weights)\n",
    "\n",
    "# Set Number of Epochs\n",
    "epochs = 10 \n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0  # Initialize counters for total loss and accuracy\n",
    "\n",
    "    total_preds = []  # Empty list to save model predictions\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.')\n",
    "\n",
    "        batch = [r.to(device) for r in batch]  # Move the batch to the device (GPU/CPU)\n",
    "\n",
    "        sent_id, mask, labels = batch  # Unpack the batch\n",
    "        \n",
    "        # # Check the data type and shape of the inputs\n",
    "        # print(f\"sent_id type: {type(sent_id)}, shape: {sent_id.shape}\")\n",
    "        # print(f\"mask type: {type(mask)}, shape: {mask.shape}\")\n",
    "        # print(f\"labels type: {type(labels)}, shape: {labels.shape}\")\n",
    "\n",
    "        model.zero_grad()  # Clear previously calculated gradients\n",
    "\n",
    "        preds = model(sent_id, mask)  # Get model predictions for the current batch\n",
    "\n",
    "        loss = cross_entropy(preds, labels)  # Compute the loss between actual and predicted values\n",
    "\n",
    "        total_loss = total_loss + loss.item()  # Accumulate the total loss\n",
    "\n",
    "        loss.backward()  # Perform a backward pass to calculate the gradients\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip the gradients to 1.0 to prevent exploding gradients\n",
    "\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()  # Detach predictions from the GPU and convert to numpy array\n",
    "\n",
    "        total_preds.append(preds)  # Append the model predictions\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)  # Compute the training loss of the epoch\n",
    "\n",
    "    total_preds = np.concatenate(total_preds, axis=0)  # Reshape the predictions\n",
    "\n",
    "    return avg_loss, total_preds  # Return the average loss and predictions\n",
    "\n",
    "# # Example usage:\n",
    "# # Assume optimizer and device are already defined\n",
    "# # model, train_dataloader are from previous snippet\n",
    "# train_loss, train_preds = train(model, train_dataloader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0  # Initialize counters for total loss and accuracy\n",
    "\n",
    "    total_preds = []  # Empty list to save model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            if step % 50 == 0 and not step == 0:\n",
    "                print(f'  Batch {step:>5,}  of  {len(val_dataloader):>5,}.')\n",
    "\n",
    "            batch = [r.to(device) for r in batch]  # Move the batch to the device (GPU/CPU)\n",
    "\n",
    "            sent_id, mask, labels = batch  # Unpack the batch\n",
    "\n",
    "            preds = model(sent_id, mask)  # Get model predictions for the current batch\n",
    "\n",
    "            loss = cross_entropy(preds, labels)  # Compute the loss between actual and predicted values\n",
    "\n",
    "            total_loss = total_loss + loss.item()  # Accumulate the total loss\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()  # Detach predictions from the GPU and convert to numpy array\n",
    "\n",
    "            total_preds.append(preds)  # Append the model predictions\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader)  # Compute the validation loss of the epoch\n",
    "\n",
    "    total_preds = np.concatenate(total_preds, axis=0)  # Reshape the predictions\n",
    "\n",
    "    return avg_loss, total_preds  # Return the average loss and predictions\n",
    "\n",
    "# # Example usage:\n",
    "# # model, val_dataloader are from previous snippet\n",
    "# val_loss, val_preds = validate(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Validation loss improved. Model saved to best_model_epoch1.pth\n",
      "Training Loss: 0.211\n",
      "Validation Loss: 0.153\n",
      "\n",
      " Epoch 2/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Validation loss improved. Model saved to best_model_epoch2.pth\n",
      "Training Loss: 0.142\n",
      "Validation Loss: 0.130\n",
      "\n",
      " Epoch 3/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Validation loss improved. Model saved to best_model_epoch3.pth\n",
      "Training Loss: 0.124\n",
      "Validation Loss: 0.120\n",
      "\n",
      " Epoch 4/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.091\n",
      "Validation Loss: 0.120\n",
      "\n",
      " Epoch 5/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.102\n",
      "Validation Loss: 0.140\n",
      "\n",
      " Epoch 6/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.083\n",
      "Validation Loss: 0.121\n",
      "\n",
      " Epoch 7/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.105\n",
      "Validation Loss: 0.135\n",
      "\n",
      " Epoch 8/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.083\n",
      "Validation Loss: 0.278\n",
      "\n",
      " Epoch 9/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Training Loss: 0.083\n",
      "Validation Loss: 0.121\n",
      "\n",
      " Epoch 10/10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "Validation loss improved. Model saved to best_model_epoch10.pth\n",
      "Training Loss: 0.086\n",
      "Validation Loss: 0.084\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"Save the model to the specified path.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def train_and_validate(model, train_dataloader, val_dataloader, optimizer, device, epochs):\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\n Epoch {epoch + 1}/{epochs}')\n",
    "        train_loss, _ = train(model, train_dataloader, optimizer, device)  # Train the model\n",
    "        val_loss, _ = validate(model, val_dataloader, device)  # Validate the model\n",
    "\n",
    "        # If the validation loss is the best seen so far, save the model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, f'./models/best_model.pth')\n",
    "            print(f'Validation loss improved. Model saved to best_model_epoch{epoch + 1}.pth')\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {val_loss:.3f}')\n",
    "\n",
    "# Example usage:\n",
    "# model, train_dataloader, val_dataloader, optimizer, device, and epochs are from previous snippets\n",
    "train_and_validate(model, train_dataloader, val_dataloader, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init the model\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# Load saved wrights\n",
    "model.load_state_dict(torch.load(\"./models/best_model.pth\"))\n",
    "\n",
    "model = model.to(device)  # Move the model to GPU if available\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predictions for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       724\n",
      "           1       0.92      0.96      0.94       112\n",
      "\n",
      "    accuracy                           0.98       836\n",
      "   macro avg       0.96      0.98      0.97       836\n",
      "weighted avg       0.98      0.98      0.98       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance, \n",
    "# since the last layer on our model was a softmax\n",
    "# We can use argmax here \n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>715</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      715    9\n",
       "1        4  108"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assume input_text is your text data\n",
    "input_text = [\"your device is infected with virous\", \"How are you doing my friend?\"]\n",
    "\n",
    "# Tokenize input text\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "    input_text,\n",
    "    max_length=25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Get tensor data\n",
    "input_ids = tokens['input_ids'].to(device)\n",
    "attention_mask = tokens['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation as we are in inference mode\n",
    "with torch.no_grad():\n",
    "    # Forward pass, get predictions\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "\n",
    "# Get predicted class indices\n",
    "_, predicted_indices = torch.max(outputs, dim=1)\n",
    "\n",
    "# Convert predicted indices to list\n",
    "predicted_indices = predicted_indices.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
